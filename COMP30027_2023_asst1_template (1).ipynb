{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2023 Semester 1\n",
    "\n",
    "## Assignment 1: Music genre classification with naive Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student ID(s):**     `PLEASE ENTER YOUR ID(S) HERE`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you will use for your Assignment 1 submission.\n",
    "\n",
    "Marking will be applied on the four functions that are defined in this notebook, and to your responses to the questions at the end of this notebook (Submitted in a separate PDF file).\n",
    "\n",
    "**NOTE: YOU SHOULD ADD YOUR RESULTS, DIAGRAMS AND IMAGES FROM YOUR OBSERVATIONS IN THIS FILE TO YOUR REPORT (the PDF file).**\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find.\n",
    "\n",
    "**Adding proper comments to your code is MANDATORY. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "data = pd.read_csv(\"pop_vs_classical_train.csv\")\n",
    "data = data.drop([\"filename\"], axis = 1)\n",
    "data\n",
    "\n",
    "def normal_dist(self, x, mean, stdev):\n",
    "    exponent = math.exp(-((x - mean) ** 2 / (2 * stdev ** 2)))\n",
    "    return (1 / (math.sqrt(2 * math.pi) * stdev)) * exponent\n",
    "\n",
    "n = len(data)\n",
    "prior_prob = {}\n",
    "labels = data.values[:,-1]\n",
    "unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "for i in range(len(unique_labels)):\n",
    "    prior_prob[unique_labels[i]] = (counts[i] / n).round(2)\n",
    "    \n",
    "unique_labels = np.unique(data.values[:,-1])\n",
    "features_list = list(data.columns)\n",
    "features_list.remove('label')\n",
    "\n",
    "parameters = pd.DataFrame(columns=['Feature', 'Mean', 'Sd'])\n",
    "\n",
    "for feature in features_list:\n",
    "    feature_values = data[feature]\n",
    "    feature_mean = np.mean(feature_values)\n",
    "    feature_sd = np.std(feature_values)\n",
    "    parameters.loc[features_list.index(feature)] = [feature, feature_mean, feature_sd]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should prepare the data by reading it from a file and converting it into a useful format for training and testing\n",
    "\n",
    "def preprocess(data):\n",
    "    data = pd.read_csv(data)\n",
    "    data = data.drop([\"filename\"], axis = 1)\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'classical': 0.51, 'pop': 0.49},\n",
       "                     Feature        Pop_mean          Pop_sd Classical_mean  \\\n",
       " 0          chroma_stft_mean        0.403086        0.055742        0.26258   \n",
       " 1           chroma_stft_var        0.089466        0.006022       0.085219   \n",
       " 2                  rms_mean        0.202279        0.058948       0.043354   \n",
       " 3                   rms_var        0.007288        0.004947       0.000724   \n",
       " 4    spectral_centroid_mean     3066.604995       599.83171     1340.38104   \n",
       " 5     spectral_centroid_var   916163.793857   390055.911803  121528.084849   \n",
       " 6   spectral_bandwidth_mean     2999.412705      327.899256     1497.10417   \n",
       " 7    spectral_bandwidth_var   177363.479891    81391.606898   77205.621052   \n",
       " 8              rolloff_mean     6637.522215     1257.985826    2445.092888   \n",
       " 9               rolloff_var  2874635.775606  1202954.962835  583126.428864   \n",
       " 10  zero_crossing_rate_mean        0.133146        0.037532       0.076817   \n",
       " 11   zero_crossing_rate_var        0.006564        0.004043       0.000817   \n",
       " 12             harmony_mean        0.000015        0.001425      -0.000225   \n",
       " 13              harmony_var        0.024283        0.014535       0.003095   \n",
       " 14            perceptr_mean        0.000005        0.000944      -0.000162   \n",
       " 15             perceptr_var        0.014124        0.008321       0.000213   \n",
       " 16                    tempo      113.736754       23.092952     128.696707   \n",
       " 17               mfcc1_mean      -65.102117       52.395941    -317.243895   \n",
       " 18                mfcc1_var      3961.84471     1751.688646    5425.085532   \n",
       " 19               mfcc2_mean       68.063223       21.480016     142.437842   \n",
       " 20                mfcc2_var      797.995952      364.091448     689.642173   \n",
       " 21               mfcc3_mean       12.306348       15.556911     -20.157578   \n",
       " 22                mfcc3_var      516.084308      202.041614     380.168987   \n",
       " 23               mfcc4_mean       16.748538        9.286203      22.627764   \n",
       " 24                mfcc4_var      280.597285       104.15644     120.343479   \n",
       " 25               mfcc5_mean       10.556319        6.270097      -2.754105   \n",
       " 26                mfcc5_var      173.818938       65.672802     126.713897   \n",
       " 27               mfcc6_mean        4.738917        5.700327       1.842665   \n",
       " 28                mfcc6_var      184.824135        69.05947      77.207161   \n",
       " 29               mfcc7_mean        6.744947        5.949939      -8.065853   \n",
       " 30                mfcc7_var      126.474043       43.649241      84.499156   \n",
       " 31               mfcc8_mean        2.735645        4.194452      -0.597215   \n",
       " 32                mfcc8_var      108.174846        38.83947      80.316834   \n",
       " 33               mfcc9_mean        4.309805        4.735945      -6.815538   \n",
       " 34                mfcc9_var       97.359679       36.763685      89.787302   \n",
       " 35              mfcc10_mean        4.788034        3.939076       0.552062   \n",
       " 36               mfcc10_var       92.036388       30.629233       87.26445   \n",
       " 37              mfcc11_mean       -1.576709        3.549734       -6.82195   \n",
       " 38               mfcc11_var       80.838816       31.301175     102.289579   \n",
       " 39              mfcc12_mean        0.575262        3.255878      -1.415902   \n",
       " 40               mfcc12_var       70.643647       25.831384      98.791478   \n",
       " 41              mfcc13_mean        0.046316        3.136648      -2.634051   \n",
       " 42               mfcc13_var       71.057612       32.237081     105.347515   \n",
       " 43              mfcc14_mean       -0.084781         3.59791      -1.077731   \n",
       " 44               mfcc14_var       70.983491       34.300586      93.975831   \n",
       " 45              mfcc15_mean        0.042074        3.094164      -0.733845   \n",
       " 46               mfcc15_var       70.295983       44.919468      84.441675   \n",
       " 47              mfcc16_mean       -0.336599        3.237679        0.79277   \n",
       " 48               mfcc16_var       72.420677       38.947072      76.452751   \n",
       " 49              mfcc17_mean        0.014973        3.562681       0.065082   \n",
       " 50               mfcc17_var       78.222551       42.683185      76.302526   \n",
       " 51              mfcc18_mean         0.82523        3.268284       0.701783   \n",
       " 52               mfcc18_var       82.827808       42.671291       81.22633   \n",
       " 53              mfcc19_mean        0.799039        2.511833      -1.125619   \n",
       " 54               mfcc19_var       81.587051        35.50534      93.398493   \n",
       " 55              mfcc20_mean        0.170252        2.908054      -1.336518   \n",
       " 56               mfcc20_var       80.165586       34.937207     111.810833   \n",
       " \n",
       "      Classical_sd  \n",
       " 0        0.040738  \n",
       " 1        0.003118  \n",
       " 2        0.036062  \n",
       " 3        0.000859  \n",
       " 4      359.291344  \n",
       " 5    97473.619731  \n",
       " 6      262.200769  \n",
       " 7    74724.334116  \n",
       " 8      777.869827  \n",
       " 9   621480.584308  \n",
       " 10       0.025647  \n",
       " 11        0.00064  \n",
       " 12       0.000645  \n",
       " 13       0.005629  \n",
       " 14       0.000403  \n",
       " 15       0.000471  \n",
       " 16       34.12889  \n",
       " 17      96.295714  \n",
       " 18    3991.199608  \n",
       " 19      24.256723  \n",
       " 20     440.283284  \n",
       " 21      20.868347  \n",
       " 22     253.267653  \n",
       " 23      11.560567  \n",
       " 24      55.327336  \n",
       " 25       6.993638  \n",
       " 26      65.497732  \n",
       " 27       8.669352  \n",
       " 28       28.46759  \n",
       " 29       4.776468  \n",
       " 30      41.296843  \n",
       " 31       9.601525  \n",
       " 32      53.632565  \n",
       " 33       5.342614  \n",
       " 34      50.126006  \n",
       " 35       7.212533  \n",
       " 36      54.371228  \n",
       " 37       3.955497  \n",
       " 38      80.655146  \n",
       " 39       5.270352  \n",
       " 40      57.139498  \n",
       " 41       4.292012  \n",
       " 42      56.649454  \n",
       " 43       4.574037  \n",
       " 44       53.74381  \n",
       " 45       4.051185  \n",
       " 46      42.596881  \n",
       " 47       4.382649  \n",
       " 48      33.254318  \n",
       " 49       4.158959  \n",
       " 50      34.166638  \n",
       " 51       3.795314  \n",
       " 52      37.881195  \n",
       " 53       3.603901  \n",
       " 54      47.083841  \n",
       " 55       3.630726  \n",
       " 56      64.236358  )"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function should calculat prior probabilities and likelihoods from the training data and using\n",
    "# them to build a naive Bayes model\n",
    "\n",
    "def train(data):\n",
    "    def normal_dist(self, x, mean, stdev):\n",
    "        exponent = math.exp(-((x - mean) ** 2 / (2 * stdev ** 2)))\n",
    "        return (1 / (math.sqrt(2 * math.pi) * stdev)) * exponent\n",
    "\n",
    "    n = len(data)\n",
    "    prior_prob = {}\n",
    "    labels = data.values[:,-1]\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    for i in range(len(unique_labels)):\n",
    "        prior_prob[unique_labels[i]] = (counts[i] / n).round(2)\n",
    "    \n",
    "    unique_labels = np.unique(data.values[:,-1])\n",
    "    features_list = list(data.columns)\n",
    "    features_list.remove('label')\n",
    "\n",
    "    parameters = pd.DataFrame(columns=['Feature', 'Pop_mean', 'Pop_sd', 'Classical_mean', 'Classical_sd'])\n",
    "\n",
    "    for feature in features_list:\n",
    "        parameters.loc[features_list.index(feature)] = [feature, 0, 0, 0, 0]\n",
    "        for label in unique_labels:\n",
    "            feature_values = data.loc[data['label'] == label][feature]\n",
    "            feature_mean = np.mean(feature_values)\n",
    "            feature_sd = np.std(feature_values)\n",
    "            if label == 'pop':\n",
    "                parameters.at[features_list.index(feature), 'Pop_mean']  = feature_mean\n",
    "                parameters.at[features_list.index(feature), 'Pop_sd'] =  feature_sd\n",
    "            else:\n",
    "                parameters.at[features_list.index(feature), 'Classical_mean'] = feature_mean\n",
    "                parameters.at[features_list.index(feature), 'Classical_sd'] = feature_sd\n",
    "    \n",
    "    return prior_prob, parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This function should predict classes for new items in a test dataset\n",
    "\n",
    "def predict():\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = [1, 2, 3]\n",
    "n.index(3)\n",
    "len(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This function should evaliate the prediction performance by comparing your modelâ€™s class outputs to ground\n",
    "# truth labels\n",
    "\n",
    "def evaluate():\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Pop vs. classical music classification\n",
    "\n",
    "#### NOTE: you may develope codes or functions to help respond to the question here, but your formal answer must be submitted separately as a PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1\n",
    "Compute and report the accuracy, precision, and recall of your model (treat \"classical\" as the \"positive\" class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2\n",
    "For each of the features X below, plot the probability density functions P(X|Class = pop) and P(X|Class = classical). If you had to classify pop vs. classical music using just one of these three features, which feature would you use and why? Refer to your plots to support your answer.\n",
    "- spectral centroid mean\n",
    "- harmony mean\n",
    "- tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. 10-way music genre classification\n",
    "\n",
    "#### NOTE: you may develope codes or functions to help respond to the question here, but your formal answer must be submitted separately as a PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3\n",
    "Compare the performance of the full model to a 0R baseline and a one-attribute baseline. The one-attribute baseline should be the best possible naive Bayes model which uses only a prior and a single attribute. In your write-up, explain how you implemented the 0R and one-attribute baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4\n",
    "Train and test your model with a range of training set sizes by setting up your own train/test splits. With each split, use cross-fold validation so you can report the performance on the entire dataset (1000 items). You may use built-in functions to set up cross-validation splits. In your write-up, evaluate how model performance changes with training set size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5\n",
    "Implement a kernel density estimate (KDE) naive Bayes model and compare its performance to your Gaussian naive Bayes model. You may use built-in functions and automatic (\"rule of thumb\") bandwidth selectors to compute the KDE probabilities, but you should implement the naive Bayes logic yourself. You should give the parameters of the KDE implementation (namely, what bandwidth(s) you used and how they were chosen) in your write-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6\n",
    "Modify your naive Bayes model to handle missing attributes in the test data. Recall from lecture that you can handle missing attributes at test by skipping the missing attributes and computing the posterior probability from the non-missing attributes. Randomly delete some attributes from the provided test set to test how robust your model is to missing data. In your write-up, evaluate how your model's performance changes as the amount of missing data increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
